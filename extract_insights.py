"""ë¸”ë¡œê·¸ ê¸€ì—ì„œ íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
import json
import os
from typing import List, Dict
import logging
from src.utils.llm import call_llm
from src.llm.models import ModelProvider
from pydantic import BaseModel, Field

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InsightMetadata(BaseModel):
    """íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„°"""
    sector: str = Field(description="ì‚°ì—… ì„¹í„° (ì˜ˆ: ê¸°ìˆ /ì–‘ìì»´í“¨íŒ…, ê±°ì‹œê²½ì œ/ê¸ˆë¦¬, ì—ë„ˆì§€/ì›ìë ¥)")
    sentiment: str = Field(description="íˆ¬ì ê°ì„± (ë§¤ìš° ê¸ì •ì /ê¸ì •ì /ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ê¸ì •ì /ì¤‘ë¦½/ì£¼ì˜/ë¶€ì •ì )")
    keywords: List[str] = Field(description="ì£¼ìš” í‚¤ì›Œë“œ 5-10ê°œ (ì¢…ëª©ëª…, ì£¼ì œ, ê¸°ìˆ  ë“±)")


def extract_metadata_from_post(post: Dict, index: int, total: int) -> Dict:
    """ê°œë³„ ê¸€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    title = post['title']
    content = post['content']

    # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ì•½ 2000ì)
    content_sample = content[:2000] if len(content) > 2000 else content

    prompt = f"""ë‹¤ìŒì€ íˆ¬ì ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ì…ë‹ˆë‹¤. ì´ ê¸€ì„ ë¶„ì„í•˜ì—¬ íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì œëª©: {title}

ë³¸ë¬¸:
{content_sample}

ìœ„ ê¸€ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
1. sector: ì–´ë–¤ ì‚°ì—… ë¶„ì•¼ì¸ì§€ (ì˜ˆ: "ê¸°ìˆ /ì–‘ìì»´í“¨íŒ…", "ê±°ì‹œê²½ì œ/ê¸ˆë¦¬", "ì—ë„ˆì§€/ì›ìë ¥", "ê¸°ìˆ /ë°˜ë„ì²´")
2. sentiment: íˆ¬ì ê°ì„± (ë§¤ìš° ê¸ì •ì , ê¸ì •ì , ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ê¸ì •ì , ì¤‘ë¦½, ì£¼ì˜, ë¶€ì •ì  ì¤‘ ì„ íƒ)
3. keywords: ì£¼ìš” í‚¤ì›Œë“œ 5-10ê°œ (ì¢…ëª©ëª…, ê¸°ìˆ ëª…, ì£¼ì œ ë“±ì„ ë°°ì—´ë¡œ)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”."""

    try:
        logger.info(f"[{index}/{total}] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘: {title[:50]}...")

        # LLM í˜¸ì¶œ (OpenAI ì‚¬ìš©)
        metadata = call_llm(
            prompt=prompt,
            model_name="gpt-4o-mini",
            model_provider=ModelProvider.OPENAI,
            pydantic_model=InsightMetadata
        )

        logger.info(f"  âœ… ì„¹í„°: {metadata.sector}, ê°ì„±: {metadata.sentiment}")
        logger.info(f"  í‚¤ì›Œë“œ: {', '.join(metadata.keywords[:5])}...")

        return {
            "sector": metadata.sector,
            "sentiment": metadata.sentiment,
            "keywords": metadata.keywords
        }

    except Exception as e:
        logger.error(f"  âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "sector": "ê¸°íƒ€",
            "sentiment": "ì¤‘ë¦½",
            "keywords": [title]
        }


def convert_to_investment_insights(raw_posts: List[Dict], output_file: str):
    """í¬ë¡¤ë§ ë°ì´í„°ë¥¼ investment_insights.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
    logger.info(f"\n" + "=" * 80)
    logger.info(f"íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹œì‘")
    logger.info(f"  - ì…ë ¥: {len(raw_posts)}ê°œ ê¸€")
    logger.info(f"  - ì¶œë ¥: {output_file}")
    logger.info(f"=" * 80 + "\n")

    insights = []
    failed_count = 0

    for i, post in enumerate(raw_posts, 1):
        try:
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = extract_metadata_from_post(post, i, len(raw_posts))

            # ë³¸ë¬¸ ìš”ì•½ (ì²˜ìŒ 500ì)
            content_summary = post['content'][:500].strip()
            if len(post['content']) > 500:
                content_summary += "..."

            # íˆ¬ì ì¸ì‚¬ì´íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            insight = {
                "id": i,
                "date": post['date'],
                "title": post['title'],
                "content": content_summary,
                "sector": metadata['sector'],
                "sentiment": metadata['sentiment'],
                "keywords": metadata['keywords'],
                "url": post['url']
            }

            insights.append(insight)

        except Exception as e:
            logger.error(f"[{i}/{len(raw_posts)}] ë³€í™˜ ì‹¤íŒ¨: {e}")
            failed_count += 1

    # JSON ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(insights, f, ensure_ascii=False, indent=2)

    logger.info(f"\n" + "=" * 80)
    logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ!")
    logger.info(f"  - ì„±ê³µ: {len(insights)}ê°œ")
    logger.info(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"  - ì €ì¥: {output_file}")
    logger.info(f"=" * 80)

    return insights


if __name__ == '__main__':
    # í¬ë¡¤ë§ëœ ì›ë³¸ ë°ì´í„° ë¡œë“œ
    raw_file = "data/blog_raw_100.json"
    output_file = "data/investment_insights_100.json"

    logger.info(f"ì›ë³¸ ë°ì´í„° ë¡œë“œ ì¤‘: {raw_file}")
    with open(raw_file, 'r', encoding='utf-8') as f:
        raw_posts = json.load(f)

    logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(raw_posts)}ê°œ ê¸€\n")

    # ë³€í™˜ ì‹¤í–‰
    insights = convert_to_investment_insights(raw_posts, output_file)

    # ìƒ˜í”Œ ì¶œë ¥
    if insights:
        logger.info(f"\nğŸ“ ìƒ˜í”Œ 3ê°œ:")
        for insight in insights[:3]:
            logger.info(f"\n{insight['id']}. {insight['title']}")
            logger.info(f"   ì„¹í„°: {insight['sector']}")
            logger.info(f"   ê°ì„±: {insight['sentiment']}")
            logger.info(f"   í‚¤ì›Œë“œ: {', '.join(insight['keywords'][:5])}")
