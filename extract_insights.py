"""ë¸”ë¡œê·¸ ê¸€ì—ì„œ íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° Supabase ìë™ ì‚½ì…"""
import json
import os
from typing import List, Dict
import logging
from src.utils.llm import call_llm
from src.llm.models import ModelProvider
from src.tools.supabase_rag import SupabaseRAG
from pydantic import BaseModel, Field

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class InsightMetadata(BaseModel):
    """íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„°"""
    sector: str = Field(description="ì‚°ì—… ì„¹í„° í•˜ë‚˜ë§Œ ì„ íƒ (ì˜ˆ: 'ê¸°ìˆ /ì–‘ìì»´í“¨íŒ…', 'ê±°ì‹œê²½ì œ/ê¸ˆë¦¬', 'ì—ë„ˆì§€/ì›ìë ¥', 'ê¸°ìˆ /ë°˜ë„ì²´')")
    sentiment: str = Field(description="íˆ¬ì ê°ì„± í•˜ë‚˜ë§Œ ì„ íƒ (ë§¤ìš° ê¸ì •ì , ê¸ì •ì , ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ê¸ì •ì , ì¤‘ë¦½, ì£¼ì˜, ë¶€ì •ì )")
    keywords: List[str] = Field(description="ì£¼ìš” í‚¤ì›Œë“œ 5-10ê°œë¥¼ ë¬¸ìì—´ ë°°ì—´ë¡œ (ì˜ˆ: ['ì‚¼ì„±ì „ì', 'HBM', 'ë°˜ë„ì²´'])")


def extract_metadata_from_post(post: Dict, index: int, total: int) -> Dict:
    """ê°œë³„ ê¸€ì—ì„œ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ"""
    title = post['title']
    content = post['content']

    # ë³¸ë¬¸ì´ ë„ˆë¬´ ê¸¸ë©´ ì•ë¶€ë¶„ë§Œ ì‚¬ìš© (ì•½ 2000ì)
    content_sample = content[:2000] if len(content) > 2000 else content

    prompt = f"""ë‹¤ìŒì€ íˆ¬ì ê´€ë ¨ ë¸”ë¡œê·¸ ê¸€ì…ë‹ˆë‹¤. ì´ ê¸€ì„ ë¶„ì„í•˜ì—¬ íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„°ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.

ì œëª©: {title}

ë³¸ë¬¸:
{content_sample}

ìœ„ ê¸€ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”:
1. sector: ê°€ì¥ ì£¼ìš”í•œ ì‚°ì—… ë¶„ì•¼ í•˜ë‚˜ë§Œ ì„ íƒ (ë¬¸ìì—´)
   ì˜ˆì‹œ: "ê¸°ìˆ /ì–‘ìì»´í“¨íŒ…", "ê±°ì‹œê²½ì œ/ê¸ˆë¦¬", "ì—ë„ˆì§€/ì›ìë ¥", "ê¸°ìˆ /ë°˜ë„ì²´", "ê¸ˆìœµ/ì€í–‰"
   ì£¼ì˜: ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”.

2. sentiment: íˆ¬ì ê°ì„± í•˜ë‚˜ë§Œ ì„ íƒ (ë¬¸ìì—´)
   ì„ íƒì§€: "ë§¤ìš° ê¸ì •ì ", "ê¸ì •ì ", "ì¡°ì‹¬ìŠ¤ëŸ½ê²Œ ê¸ì •ì ", "ì¤‘ë¦½", "ì£¼ì˜", "ë¶€ì •ì "
   ì£¼ì˜: ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ë¬¸ìì—´ë§Œ ë°˜í™˜í•˜ì„¸ìš”.

3. keywords: ì£¼ìš” í‚¤ì›Œë“œ 5-10ê°œë¥¼ ë¬¸ìì—´ ë°°ì—´ë¡œ
   ì˜ˆì‹œ: ["ì‚¼ì„±ì „ì", "HBM", "ë°˜ë„ì²´", "AI", "ì—”ë¹„ë””ì•„"]

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{"sector": "ë¬¸ìì—´", "sentiment": "ë¬¸ìì—´", "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}}
"""

    try:
        logger.info(f"[{index}/{total}] ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì¤‘: {title[:50]}...")

        # LLM í˜¸ì¶œ (Ollama ë¡œì»¬ ì‚¬ìš© - Mistral Small 3.1)
        metadata = call_llm(
            prompt=prompt,
            model_name="mistral-small3.1",
            model_provider=ModelProvider.OLLAMA,
            pydantic_model=InsightMetadata
        )

        # sectorì™€ sentimentê°€ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜ëœ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš©
        sector = metadata.sector if isinstance(metadata.sector, str) else (metadata.sector[0] if metadata.sector else "ê¸°íƒ€")
        sentiment = metadata.sentiment if isinstance(metadata.sentiment, str) else (metadata.sentiment[0] if metadata.sentiment else "ì¤‘ë¦½")
        keywords = metadata.keywords if isinstance(metadata.keywords, list) else [str(metadata.keywords)]

        logger.info(f"  âœ… ì„¹í„°: {sector}, ê°ì„±: {sentiment}")
        logger.info(f"  í‚¤ì›Œë“œ: {', '.join(keywords[:5])}...")

        return {
            "sector": sector,
            "sentiment": sentiment,
            "keywords": keywords
        }

    except Exception as e:
        logger.error(f"  âŒ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        # ê¸°ë³¸ê°’ ë°˜í™˜
        return {
            "sector": "ê¸°íƒ€",
            "sentiment": "ì¤‘ë¦½",
            "keywords": [title]
        }


def convert_to_investment_insights(raw_posts: List[Dict], output_file: str, enable_supabase: bool = True):
    """í¬ë¡¤ë§ ë°ì´í„°ë¥¼ investment_insights.json í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ë° Supabase ìë™ ì‚½ì…"""
    logger.info(f"\n" + "=" * 80)
    logger.info(f"íˆ¬ì ì¸ì‚¬ì´íŠ¸ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° Supabase ì‚½ì… ì‹œì‘")
    logger.info(f"  - ì…ë ¥: {len(raw_posts)}ê°œ ê¸€")
    logger.info(f"  - ì¶œë ¥: {output_file}")
    logger.info(f"  - Supabase ìë™ ì‚½ì…: {'í™œì„±í™”' if enable_supabase else 'ë¹„í™œì„±í™”'}")
    logger.info(f"=" * 80 + "\n")

    # Supabase RAG ì´ˆê¸°í™”
    rag = SupabaseRAG() if enable_supabase else None

    insights = []
    failed_count = 0
    supabase_success = 0
    supabase_failed = 0

    for i, post in enumerate(raw_posts, 1):
        try:
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            metadata = extract_metadata_from_post(post, i, len(raw_posts))

            # ë³¸ë¬¸ ì „ì²´ ì‚¬ìš© (Supabase ì €ì¥ìš©)
            content_full = post['content'].strip()

            # íˆ¬ì ì¸ì‚¬ì´íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            insight = {
                "id": i,
                "date": post['date'],
                "title": post['title'],
                "content": content_full,
                "sector": metadata['sector'],
                "sentiment": metadata['sentiment'],
                "keywords": metadata['keywords'],
                "url": post['url']
            }

            insights.append(insight)

            # Supabase ìë™ ì‚½ì…
            if rag:
                try:
                    rag.insert_insight(insight)
                    supabase_success += 1
                    logger.info(f"  âœ… Supabase ì‚½ì… ì™„ë£Œ ({supabase_success}/{i})")
                except Exception as e:
                    supabase_failed += 1
                    logger.error(f"  âŒ Supabase ì‚½ì… ì‹¤íŒ¨: {e}")

            # 10ê°œë§ˆë‹¤ ì¤‘ê°„ ì €ì¥
            if i % 10 == 0:
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(insights, f, ensure_ascii=False, indent=2)
                logger.info(f"  ğŸ’¾ ì¤‘ê°„ ì €ì¥ ì™„ë£Œ: {i}ê°œ ì²˜ë¦¬")

        except Exception as e:
            logger.error(f"[{i}/{len(raw_posts)}] ë³€í™˜ ì‹¤íŒ¨: {e}")
            failed_count += 1

    # ìµœì¢… JSON ì €ì¥
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(insights, f, ensure_ascii=False, indent=2)

    logger.info(f"\n" + "=" * 80)
    logger.info(f"âœ… ë³€í™˜ ì™„ë£Œ!")
    logger.info(f"  - ì„±ê³µ: {len(insights)}ê°œ")
    logger.info(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
    if rag:
        logger.info(f"  - Supabase ì‚½ì… ì„±ê³µ: {supabase_success}ê°œ")
        logger.info(f"  - Supabase ì‚½ì… ì‹¤íŒ¨: {supabase_failed}ê°œ")
    logger.info(f"  - ì €ì¥: {output_file}")
    logger.info(f"=" * 80)

    return insights


if __name__ == '__main__':
    # í¬ë¡¤ë§ëœ ì›ë³¸ ë°ì´í„° ë¡œë“œ
    raw_file = "data/blog_raw_100.json"
    output_file = "data/investment_insights_100.json"

    logger.info(f"ì›ë³¸ ë°ì´í„° ë¡œë“œ ì¤‘: {raw_file}")
    with open(raw_file, 'r', encoding='utf-8') as f:
        raw_posts = json.load(f)

    logger.info(f"ë¡œë“œ ì™„ë£Œ: {len(raw_posts)}ê°œ ê¸€\n")

    # ë³€í™˜ ì‹¤í–‰
    insights = convert_to_investment_insights(raw_posts, output_file)

    # ìƒ˜í”Œ ì¶œë ¥
    if insights:
        logger.info(f"\nğŸ“ ìƒ˜í”Œ 3ê°œ:")
        for insight in insights[:3]:
            logger.info(f"\n{insight['id']}. {insight['title']}")
            logger.info(f"   ì„¹í„°: {insight['sector']}")
            logger.info(f"   ê°ì„±: {insight['sentiment']}")
            logger.info(f"   í‚¤ì›Œë“œ: {', '.join(insight['keywords'][:5])}")
