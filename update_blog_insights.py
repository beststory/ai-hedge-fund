"""ë¸”ë¡œê·¸ ì‹ ê·œ ê¸€ ìë™ ì—…ë°ì´íŠ¸ ìŠ¤í¬ë¦½íŠ¸ - ë§¤ì¼ ìƒˆë²½ 3ì‹œ ì‹¤í–‰"""
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from src.tools.blog_crawler import NaverBlogCrawler
from src.tools.supabase_rag import SupabaseRAG
from extract_insights import extract_metadata_from_post

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/update_blog_insights.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def get_latest_post_date_from_supabase() -> Optional[str]:
    """Supabaseì—ì„œ ê°€ì¥ ìµœê·¼ ê¸€ì˜ ë‚ ì§œ ì¡°íšŒ"""
    try:
        rag = SupabaseRAG()

        # investment_insights í…Œì´ë¸”ì—ì„œ ìµœì‹  ë‚ ì§œ ì¡°íšŒ
        response = rag.client.table('investment_insights') \
            .select('date') \
            .order('date', desc=True) \
            .limit(1) \
            .execute()

        if response.data and len(response.data) > 0:
            latest_date = response.data[0]['date']
            logger.info(f"ğŸ“… Supabase ìµœì‹  ê¸€ ë‚ ì§œ: {latest_date}")
            return latest_date
        else:
            logger.warning("âš ï¸  Supabaseì— ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return None

    except Exception as e:
        logger.error(f"âŒ Supabase ìµœì‹  ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return None


def parse_relative_time(date_str: str) -> Optional[datetime]:
    """ìƒëŒ€ ì‹œê°„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
    if not date_str:
        return None

    try:
        # ì ˆëŒ€ ì‹œê°„ í˜•ì‹ (ì˜ˆ: "2025. 9. 9. 11:00")
        if '.' in date_str and ':' in date_str:
            return datetime.strptime(date_str.strip(), '%Y. %m. %d. %H:%M')

        # ìƒëŒ€ ì‹œê°„ í˜•ì‹ (ì˜ˆ: "6ì‹œê°„ ì „", "3ì¼ ì „")
        now = datetime.now()
        if 'ì‹œê°„ ì „' in date_str:
            hours = int(date_str.replace('ì‹œê°„ ì „', '').strip())
            return now - timedelta(hours=hours)
        elif 'ì¼ ì „' in date_str:
            days = int(date_str.replace('ì¼ ì „', '').strip())
            return now - timedelta(days=days)
        elif 'ë¶„ ì „' in date_str:
            minutes = int(date_str.replace('ë¶„ ì „', '').strip())
            return now - timedelta(minutes=minutes)

        return None
    except Exception as e:
        logger.warning(f"âš ï¸  ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨: {date_str} - {e}")
        return None


def crawl_new_posts(blog_id: str, since_date: Optional[str] = None) -> List[Dict]:
    """ì‹ ê·œ ë¸”ë¡œê·¸ ê¸€ í¬ë¡¤ë§"""
    logger.info(f"ğŸ” ì‹ ê·œ ê¸€ í¬ë¡¤ë§ ì‹œì‘ (ë¸”ë¡œê·¸: {blog_id})")
    if since_date:
        logger.info(f"  - ê¸°ì¤€ ë‚ ì§œ: {since_date} ì´í›„")

    crawler = NaverBlogCrawler(blog_id)
    new_posts = []

    try:
        # ìµœê·¼ 100ê°œ ê¸€ë§Œ ì²´í¬ (ë³´í†µ í•˜ë£¨ì— ëª‡ ê°œ ì•ˆ ì˜¬ë¼ì˜´)
        all_posts = crawler.crawl_all(max_posts=100, save_path=None)

        if not since_date:
            # ê¸°ì¤€ ë‚ ì§œê°€ ì—†ìœ¼ë©´ ëª¨ë‘ ìƒˆ ê¸€
            new_posts = all_posts
        else:
            # ê¸°ì¤€ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
            since_datetime = parse_relative_time(since_date)

            if not since_datetime:
                # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ëª¨ë“  ê¸€ì„ ìƒˆ ê¸€ë¡œ ê°„ì£¼
                logger.warning("âš ï¸  ê¸°ì¤€ ë‚ ì§œ íŒŒì‹± ì‹¤íŒ¨, ëª¨ë“  ê¸€ì„ ìƒˆ ê¸€ë¡œ ê°„ì£¼í•©ë‹ˆë‹¤.")
                new_posts = all_posts
            else:
                # ê¸°ì¤€ ë‚ ì§œ ì´í›„ ê¸€ë§Œ í•„í„°ë§
                for post in all_posts:
                    post_date_str = post.get('date', '')
                    post_datetime = parse_relative_time(post_date_str)

                    if post_datetime and post_datetime > since_datetime:
                        new_posts.append(post)
                        logger.debug(f"  - ì‹ ê·œ ê¸€ ë°œê²¬: {post['title'][:50]}... ({post_date_str})")

        logger.info(f"âœ… ì‹ ê·œ ê¸€ {len(new_posts)}ê°œ ë°œê²¬")

    except Exception as e:
        logger.error(f"âŒ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
    finally:
        crawler.close()

    return new_posts


def process_and_upload_new_posts(new_posts: List[Dict]) -> Dict[str, int]:
    """ì‹ ê·œ ê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° Supabase ì—…ë¡œë“œ"""
    if not new_posts:
        logger.info("â„¹ï¸  ì‹ ê·œ ê¸€ì´ ì—†ìŠµë‹ˆë‹¤.")
        return {"success": 0, "failed": 0}

    logger.info(f"\n{'=' * 80}")
    logger.info(f"ğŸ¤– ì‹ ê·œ ê¸€ ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ë° ì—…ë¡œë“œ ì‹œì‘: {len(new_posts)}ê°œ")
    logger.info(f"{'=' * 80}\n")

    rag = SupabaseRAG()
    success_count = 0
    failed_count = 0

    # ìµœì‹  ID ì¡°íšŒ (ì´ì–´ì„œ ë²ˆí˜¸ ë¶€ì—¬)
    try:
        response = rag.client.table('investment_insights') \
            .select('id') \
            .order('id', desc=True) \
            .limit(1) \
            .execute()

        next_id = response.data[0]['id'] + 1 if response.data else 1
    except:
        next_id = 1

    for i, post in enumerate(new_posts, 1):
        try:
            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
            logger.info(f"[{i}/{len(new_posts)}] ì²˜ë¦¬ ì¤‘: {post['title'][:50]}...")
            metadata = extract_metadata_from_post(post, i, len(new_posts))

            # íˆ¬ì ì¸ì‚¬ì´íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            insight = {
                "id": next_id,
                "date": post['date'],
                "title": post['title'],
                "content": post['content'].strip(),
                "sector": metadata['sector'],
                "sentiment": metadata['sentiment'],
                "keywords": metadata['keywords'],
                "url": post['url']
            }

            # Supabase ì‚½ì…
            rag.insert_insight(insight)
            success_count += 1
            next_id += 1

            logger.info(f"  âœ… ì—…ë¡œë“œ ì™„ë£Œ: {post['title'][:50]}...")
            logger.info(f"     ì„¹í„°: {metadata['sector']}, ê°ì„±: {metadata['sentiment']}")

        except Exception as e:
            failed_count += 1
            logger.error(f"  âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

    logger.info(f"\n{'=' * 80}")
    logger.info(f"âœ… ì‹ ê·œ ê¸€ ì—…ë°ì´íŠ¸ ì™„ë£Œ!")
    logger.info(f"  - ì„±ê³µ: {success_count}ê°œ")
    logger.info(f"  - ì‹¤íŒ¨: {failed_count}ê°œ")
    logger.info(f"{'=' * 80}\n")

    return {"success": success_count, "failed": failed_count}


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    start_time = datetime.now()
    logger.info(f"\n{'=' * 80}")
    logger.info(f"ğŸš€ ë¸”ë¡œê·¸ ì‹ ê·œ ê¸€ ì—…ë°ì´íŠ¸ ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"{'=' * 80}\n")

    blog_id = "ranto28"

    # 1. Supabaseì—ì„œ ìµœì‹  ê¸€ ë‚ ì§œ ì¡°íšŒ
    latest_date = get_latest_post_date_from_supabase()

    # 2. ì‹ ê·œ ê¸€ í¬ë¡¤ë§
    new_posts = crawl_new_posts(blog_id, since_date=latest_date)

    # 3. ì‹ ê·œ ê¸€ ì²˜ë¦¬ ë° ì—…ë¡œë“œ
    result = process_and_upload_new_posts(new_posts)

    # ì‹¤í–‰ ì‹œê°„ ê³„ì‚°
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    logger.info(f"\n{'=' * 80}")
    logger.info(f"ğŸ‰ ì—…ë°ì´íŠ¸ ì‘ì—… ì™„ë£Œ!")
    logger.info(f"  - ì‹¤í–‰ ì‹œê°„: {duration:.1f}ì´ˆ")
    logger.info(f"  - ì‹ ê·œ ê¸€: {len(new_posts)}ê°œ")
    logger.info(f"  - ì„±ê³µ: {result['success']}ê°œ")
    logger.info(f"  - ì‹¤íŒ¨: {result['failed']}ê°œ")
    logger.info(f"{'=' * 80}\n")


if __name__ == '__main__':
    main()
