"""ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ - íˆ¬ì ì¸ì‚¬ì´íŠ¸ ìˆ˜ì§‘"""
import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime
from typing import List, Dict
import logging
from urllib.parse import unquote_plus

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class NaverBlogCrawler:
    """ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ëŸ¬ (ëª¨ë°”ì¼ ë²„ì „ ì‚¬ìš©)"""

    def __init__(self, blog_id: str):
        self.blog_id = blog_id
        self.base_url = f"https://blog.naver.com/{blog_id}"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'
        })

    def get_post_list(self, max_posts: int = 1877) -> List[str]:
        """ë¸”ë¡œê·¸ì˜ ëª¨ë“  ê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
        logger.info(f"ğŸ“š ë¸”ë¡œê·¸ ê¸€ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘... (ìµœëŒ€ {max_posts}ê°œ)")

        post_urls = []
        page = 1

        while len(post_urls) < max_posts:
            try:
                # ë„¤ì´ë²„ ë¸”ë¡œê·¸ ëª©ë¡ API ì‚¬ìš© (JSON ì‘ë‹µ)
                list_url = f"https://blog.naver.com/PostTitleListAsync.naver?blogId={self.blog_id}&viewdate=&currentPage={page}&categoryNo=&parentCategoryNo=&countPerPage=30"

                response = self.session.get(list_url, timeout=10)

                if response.status_code != 200:
                    logger.warning(f"í˜ì´ì§€ {page} ë¡œë“œ ì‹¤íŒ¨: {response.status_code}")
                    break

                # JSON íŒŒì‹± (ì˜ëª»ëœ ì´ìŠ¤ì¼€ì´í”„ ìˆ˜ì •)
                fixed_text = response.text.replace("\\'", "'")
                data = json.loads(fixed_text)

                if data.get('resultCode') != 'S':
                    logger.warning(f"í˜ì´ì§€ {page} API ì˜¤ë¥˜: {data.get('resultMessage')}")
                    break

                post_list = data.get('postList', [])

                if not post_list:
                    logger.info(f"í˜ì´ì§€ {page}ì—ì„œ ë” ì´ìƒ ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    break

                # ê¸€ URL ìƒì„±
                for post in post_list:
                    log_no = post.get('logNo')
                    if log_no:
                        post_url = f"https://blog.naver.com/{self.blog_id}/{log_no}"
                        post_urls.append(post_url)

                logger.info(f"í˜ì´ì§€ {page}: {len(post_list)}ê°œ ê¸€ ë°œê²¬ (ì´ {len(post_urls)}ê°œ)")

                page += 1
                time.sleep(0.5)  # Rate limiting

            except Exception as e:
                logger.error(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")
                break

        logger.info(f"âœ… ì´ {len(post_urls)}ê°œ ê¸€ URL ìˆ˜ì§‘ ì™„ë£Œ")
        return post_urls[:max_posts]

    def get_post_content(self, post_url: str) -> Dict:
        """ê°œë³„ ê¸€ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°"""
        try:
            # ëª¨ë°”ì¼ ë²„ì „ URL ì‚¬ìš©
            log_no = post_url.split('/')[-1]
            mobile_url = f"https://m.blog.naver.com/{self.blog_id}/{log_no}"

            response = self.session.get(mobile_url, timeout=10)

            if response.status_code != 200:
                return None

            soup = BeautifulSoup(response.text, 'html.parser')

            # ì œëª© ì¶”ì¶œ (se-documentTitle í´ë˜ìŠ¤ ì‚¬ìš©)
            title = "ì œëª© ì—†ìŒ"
            title_container = soup.find('div', class_='se-documentTitle')
            if title_container:
                title_span = title_container.find('span', class_=lambda x: x and 'se-fs-' in str(x))
                if title_span:
                    title = title_span.get_text(strip=True)
                else:
                    # ëŒ€ì²´ ë°©ë²•: p íƒœê·¸ì—ì„œ ì²« ë²ˆì§¸ í…ìŠ¤íŠ¸
                    content_elem = title_container.find('div', class_='se-component-content')
                    if content_elem:
                        p_tag = content_elem.find('p')
                        if p_tag:
                            title = p_tag.get_text(strip=True)

            # ë³¸ë¬¸ ì¶”ì¶œ (se-text ì»´í¬ë„ŒíŠ¸ë“¤ì—ì„œ)
            content_parts = []
            post_view = soup.find('div', class_='_postView')
            if post_view:
                main_container = post_view.find('div', class_='se-main-container')
                if main_container:
                    # se-text í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ì»´í¬ë„ŒíŠ¸ë“¤ ì°¾ê¸°
                    text_components = main_container.find_all('div', class_='se-text')
                    for component in text_components:
                        text = component.get_text(separator='\n', strip=True)
                        # ì˜ë¯¸ ìˆëŠ” ê¸¸ì´ì˜ í…ìŠ¤íŠ¸ë§Œ ì¶”ê°€
                        if text and len(text) > 10:
                            content_parts.append(text)

            content = '\n\n'.join(content_parts)

            # ì‘ì„±ì¼ ì¶”ì¶œ
            date_str = ""
            # ë°©ë²• 1: blog_date í´ë˜ìŠ¤
            date_elem = soup.find('span', class_='blog_date')
            if date_elem:
                date_str = date_elem.get_text(strip=True)

            # ë°©ë²• 2: se-publishDate í´ë˜ìŠ¤ (documentTitle ì•ˆ)
            if not date_str and title_container:
                date_p = title_container.find_all('p')
                if len(date_p) > 1:
                    date_str = date_p[1].get_text(strip=True)

            return {
                'url': post_url,
                'title': title,
                'content': content,
                'date': date_str,
                'crawled_at': datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"ê¸€ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({post_url}): {e}")
            return None

    def crawl_all(self, max_posts: int = 100, save_path: str = None) -> List[Dict]:
        """ì „ì²´ ë¸”ë¡œê·¸ í¬ë¡¤ë§"""
        logger.info(f"ğŸš€ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘: {self.blog_id}")

        # 1. ê¸€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        post_urls = self.get_post_list(max_posts)

        if not post_urls:
            logger.error("ê¸€ ëª©ë¡ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        # 2. ê° ê¸€ ë‚´ìš© í¬ë¡¤ë§
        posts = []
        failed_count = 0

        for i, url in enumerate(post_urls, 1):
            logger.info(f"[{i}/{len(post_urls)}] í¬ë¡¤ë§ ì¤‘: {url}")

            post_data = self.get_post_content(url)

            if post_data:
                posts.append(post_data)
                logger.info(f"  âœ… ì œëª©: {post_data['title'][:50]}...")
            else:
                failed_count += 1
                logger.warning(f"  âŒ ì‹¤íŒ¨")

            # Rate limiting
            time.sleep(1)

            # ì§„í–‰ ìƒí™© ì €ì¥ (100ê°œë§ˆë‹¤)
            if save_path and i % 100 == 0:
                self._save_progress(posts, save_path)

        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(posts)}ê°œ ì„±ê³µ, {failed_count}ê°œ ì‹¤íŒ¨")

        # ìµœì¢… ì €ì¥
        if save_path:
            self._save_progress(posts, save_path)

        return posts

    def _save_progress(self, posts: List[Dict], save_path: str):
        """ì§„í–‰ ìƒí™© ì €ì¥"""
        try:
            with open(save_path, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ’¾ ì§„í–‰ ìƒí™© ì €ì¥: {save_path} ({len(posts)}ê°œ)")
        except Exception as e:
            logger.error(f"ì €ì¥ ì‹¤íŒ¨: {e}")

    def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        self.session.close()


def test_crawler():
    """í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    crawler = NaverBlogCrawler("ranto28")

    try:
        # ë¨¼ì € 10ê°œë§Œ í…ŒìŠ¤íŠ¸
        logger.info("ğŸ§ª 10ê°œ ê¸€ í…ŒìŠ¤íŠ¸ í¬ë¡¤ë§...")
        posts = crawler.crawl_all(max_posts=10, save_path="/tmp/blog_test.json")

        if posts:
            logger.info(f"\nğŸ“Š í¬ë¡¤ë§ ê²°ê³¼:")
            logger.info(f"  - ì´ ê¸€ ìˆ˜: {len(posts)}")
            logger.info(f"  - ì²« ë²ˆì§¸ ê¸€ ì œëª©: {posts[0]['title']}")
            logger.info(f"  - ë³¸ë¬¸ ê¸¸ì´ í‰ê· : {sum(len(p['content']) for p in posts) / len(posts):.0f}ì")

            # ìƒ˜í”Œ ì¶œë ¥
            logger.info(f"\nğŸ“„ ì²« ë²ˆì§¸ ê¸€ ìƒ˜í”Œ:")
            logger.info(f"  ì œëª©: {posts[0]['title']}")
            logger.info(f"  ë‚ ì§œ: {posts[0]['date']}")
            logger.info(f"  ë‚´ìš© (ì²˜ìŒ 200ì): {posts[0]['content'][:200]}...")

            return True
        else:
            logger.error("í¬ë¡¤ë§ ì‹¤íŒ¨")
            return False
    finally:
        crawler.close()


if __name__ == '__main__':
    test_crawler()
