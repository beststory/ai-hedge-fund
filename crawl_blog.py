"""ë„¤ì´ë²„ ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"""
import sys
from src.tools.blog_crawler import NaverBlogCrawler
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

if __name__ == '__main__':
    blog_id = "ranto28"

    # í¬ë¡¤ë§í•  ê¸€ ê°œìˆ˜ (ê¸°ë³¸: 100ê°œ, ì „ì²´: 1877ê°œ)
    max_posts = int(sys.argv[1]) if len(sys.argv) > 1 else 100

    output_file = f"data/blog_raw_{max_posts}.json"

    logger.info(f"=" * 80)
    logger.info(f"ë¸”ë¡œê·¸ í¬ë¡¤ë§ ì‹œì‘")
    logger.info(f"  - ë¸”ë¡œê·¸ ID: {blog_id}")
    logger.info(f"  - ìµœëŒ€ ê¸€ ìˆ˜: {max_posts}ê°œ")
    logger.info(f"  - ì¶œë ¥ íŒŒì¼: {output_file}")
    logger.info(f"  - ì˜ˆìƒ ì‹œê°„: ì•½ {max_posts * 1.5 / 60:.1f}ë¶„")
    logger.info(f"=" * 80)

    crawler = NaverBlogCrawler(blog_id)

    try:
        posts = crawler.crawl_all(max_posts=max_posts, save_path=output_file)

        logger.info(f"\n" + "=" * 80)
        logger.info(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ!")
        logger.info(f"  - ì„±ê³µ: {len(posts)}ê°œ")
        logger.info(f"  - ì €ì¥: {output_file}")
        logger.info(f"=" * 80)

        # í†µê³„ ì¶œë ¥
        if posts:
            avg_title_len = sum(len(p['title']) for p in posts) / len(posts)
            avg_content_len = sum(len(p['content']) for p in posts) / len(posts)

            logger.info(f"\nğŸ“Š í†µê³„:")
            logger.info(f"  - í‰ê·  ì œëª© ê¸¸ì´: {avg_title_len:.0f}ì")
            logger.info(f"  - í‰ê·  ë³¸ë¬¸ ê¸¸ì´: {avg_content_len:.0f}ì")

            # ìƒ˜í”Œ 3ê°œ ì¶œë ¥
            logger.info(f"\nğŸ“ ìƒ˜í”Œ 3ê°œ:")
            for i, post in enumerate(posts[:3], 1):
                logger.info(f"\n{i}. {post['title']}")
                logger.info(f"   ë‚ ì§œ: {post['date']}")
                logger.info(f"   ë³¸ë¬¸: {post['content'][:100]}...")

    finally:
        crawler.close()
