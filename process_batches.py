"""í¬ë¡¤ë§ëœ ë¸”ë¡œê·¸ë¥¼ 100ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ - ë°±ê·¸ë¼ìš´ë“œ ì‹¤í–‰ìš©"""
import json
import sys
import logging
from pathlib import Path
from extract_insights import convert_to_investment_insights

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/tmp/process_batches.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def split_into_batches(all_posts, batch_size=100):
    """ë°ì´í„°ë¥¼ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°"""
    batches = []
    for i in range(0, len(all_posts), batch_size):
        batches.append(all_posts[i:i+batch_size])
    return batches

if __name__ == '__main__':
    input_file = "data/blog_raw_all.json"
    batch_size = 100

    # ì…ë ¥ íŒŒì¼ í™•ì¸
    if not Path(input_file).exists():
        logger.error(f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_file}")
        logger.info("ë¨¼ì € crawl_all_blogs.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        sys.exit(1)

    # ì „ì²´ ë°ì´í„° ë¡œë“œ
    logger.info(f"ğŸ“– ë°ì´í„° ë¡œë“œ ì¤‘: {input_file}")
    with open(input_file, 'r', encoding='utf-8') as f:
        all_posts = json.load(f)

    logger.info(f"âœ… ì´ {len(all_posts)}ê°œ ë¸”ë¡œê·¸ ê¸€ ë¡œë“œ ì™„ë£Œ")

    # ë°°ì¹˜ë¡œ ë‚˜ëˆ„ê¸°
    batches = split_into_batches(all_posts, batch_size)
    logger.info(f"ğŸ“¦ {len(batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í•  ì™„ë£Œ (ë°°ì¹˜ë‹¹ {batch_size}ê°œ)")

    # ê° ë°°ì¹˜ ì²˜ë¦¬
    all_insights = []
    for i, batch in enumerate(batches, 1):
        logger.info(f"\n{'='*80}")
        logger.info(f"ë°°ì¹˜ {i}/{len(batches)} ì²˜ë¦¬ ì¤‘ (ê¸€ {(i-1)*batch_size+1} ~ {(i-1)*batch_size+len(batch)})")
        logger.info(f"{'='*80}\n")

        try:
            # ë°°ì¹˜ë³„ ì¶œë ¥ íŒŒì¼
            batch_output = f"data/investment_insights_batch_{i}.json"

            # ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ ë° Supabase ì‚½ì…
            insights = convert_to_investment_insights(
                batch,
                batch_output,
                enable_supabase=True  # Supabase ìë™ ì‚½ì… í™œì„±í™”
            )

            all_insights.extend(insights)
            logger.info(f"âœ… ë°°ì¹˜ {i} ì™„ë£Œ: {len(insights)}ê°œ ì¸ì‚¬ì´íŠ¸ ì¶”ì¶œ")

        except Exception as e:
            logger.error(f"âŒ ë°°ì¹˜ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    # ì „ì²´ ê²°ê³¼ ì €ì¥
    final_output = "data/investment_insights_all.json"
    with open(final_output, 'w', encoding='utf-8') as f:
        json.dump(all_insights, f, ensure_ascii=False, indent=2)

    logger.info(f"\n{'='*80}")
    logger.info(f"ğŸ‰ ì „ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
    logger.info(f"  - ì´ ì¸ì‚¬ì´íŠ¸: {len(all_insights)}ê°œ")
    logger.info(f"  - ìµœì¢… íŒŒì¼: {final_output}")
    logger.info(f"{'='*80}")
